---
title: Rain in Australia. Classification Prediction Model
author:
  - name: Sumaira Afzal
    affiliation: York University School of Continuing Studies
  - name: Viraja Ketkar
    affiliation: York University School of Continuing Studies
  - name: Murlidhar Loka
    affiliation: York University School of Continuing Studies
  - name: Vadim Spirkov
    affiliation: York University School of Continuing Studies
abstract: >
  An abstract of less than 150 words.
output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
   
---

```{r echo=FALSE, message=FALSE, warnings=FALSE}
# load required libraries
library(ggplot2) # plotting lib
library(gridExtra) # arrange grids
library(dplyr)  # data manipuation
library(mice)  # data imputing
library(corrplot) # correlation matrix plotting/printing
library(caret) # predictive models
library(pROC) # to measure model performance
library(leaflet) # maps
library(RColorBrewer) # color palettes
library(VIM) # missing value analysis 
library(lattice) # another data plotting library
library(mapview) # saves map objects as file
library(png) # deals with png file measurements
library(knitr) #

# Clean all variables that might be left by other script to avoid collusion
rm(list=ls(all=TRUE))
# pick palettes
mainPalette = brewer.pal(8,"Dark2")
# set a sample size
SampleSize = 20000
```

```{r global_options, include=FALSE}
# make the images flow nicely
knitr::opts_chunk$set(fig.pos = 'H')
```

## Introduction


## Background


## Objective


# Data Analisys

The data set we are going to use for our research contains daily weather observations from numerous Australian weather stations from 2007 till 2017. There are over 142000 records. It has been sourced from [Kagle](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package)


## Data Dictionary

We exclude the variable Risk-MM when training your binary classification model. If we don't exclude it, you will leak the answers to our model and reduce its predictability

Column Name            | Column Description  
-----------------------| ------------------- 
Date                   | Date of observation 
Location               | Common name of the location of the weather station
MinTemp                | Minimum temperature in degrees Celsius
MaxTemp                | Maximum temperature in degrees Celsius
Rainfall               | Amount of rainfall recorded for the day in mm
Evaporation            | So-called Class A pan evaporation (mm) in the 24 hours to 9am
Sunshine               | Number of hours of bright sunshine in the day
WindGustDir            | Direction of the strongest wind gust in the 24 hours to midnight
WindGustSpeed          | Speed (km/h) of the strongest wind gust in the 24 hours to midnight
WindDir9amDirection    | Of the wind at 9am
WindDir3pmDirection    | Of the wind at 3pm
WindSpeed9amWind       | Wind speed (km/hr) averaged over 10 minutes prior to 9am
WindSpeed3pmWind       | Wind Speed (km/hr) averaged over 10 minutes prior to 3pm
Humidity9amHumidity    |Humidity (percent) at 9am
Humidity3pmHumidity    | Humidity (percent) at 3pm
Pressure9amAtmospheric | Pressure (hpa) reduced to mean sea level at 9am
Pressure3pmAtmospheric | Pressure (hpa) reduced to mean sea level at 3pm
Cloud9amFraction       | Area of sky obscured by cloud at 9am. This is measured in "oktas", which are a unit of eights. It records how many eights of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast
Cloud3pmFraction       | Area of sky obscured by cloud (in "oktas": eighths) at 3pm. See Cloud9am for a description of the values
Temp9amTemperature     |Temperature (degrees C) at 9am
Temp3pmTemperature     |Temperature (degrees C) at 3pm
RainTodayBoolean       | Rainy today. 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0
RISK_MM                | Amount of rain. A kind of measure of the "risk". This column is redundant and will be dropped
**RainTomorrowThe**    | **Target variable. Will it rain tomorrow?**


## Data Exploration

Let's take a close look at the data set. We start with loading weather observations from the file into a data frame. We remove RISK_MM as explained and convert Date column to *date* format 

```{r message=FALSE, warning=FALSE}
weatherData = read.csv("../data/weatherAUS.csv", header = TRUE, na.strings = c("NA","","#NA"),sep=",")
weatherData = subset(weatherData, select = -RISK_MM)
weatherData$Date = as.Date(as.character(weatherData$Date),"%Y-%m-%d")
```
  
Now let's load coordinates of the weather stations and have a bird-eye view of the weather station locations

```{r map, dpi = 100, echo=FALSE, fig.cap="Australian Weather Stations", message=FALSE, warning=FALSE, out.width="1.1\\linewidth", include=TRUE, fig.align="center"}
locations = read.csv("../data/AusCoordinates.csv", header = TRUE, sep=",")
# The code that generates the map has been excluded for performance sake
# map = leaflet() %>% setView(lng = 133.8836, lat = -23.69748, zoom = 4 ) %>% addTiles() %>% 
#  addCircleMarkers(data = locations, lng = ~Longtitude, lat = ~Latitude,fillColor = mainPallet[1], 
#                   color = mainPallet[1],  label =~Location,  
#                   labelOptions = labelOptions(noHide = T, textOnly = T))
# mapshot(map, file ="images/weatherStations.png")
knitr::include_graphics("images/weatherStations.png")
```

Let's review data summary

```{r} 
summary(weatherData)
```

Next set of plots renders distribution of a few selected features.

```{r feature_distribution, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Observations Distribution"}

tmp = weatherData %>%filter(complete.cases(.))

p1 = tmp %>% ggplot(aes(x=Evaporation  )) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2)
p2 = tmp %>% ggplot(aes(x=MaxTemp  )) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2)
p3 = tmp %>% ggplot(aes(x=Pressure9am  )) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2)
p4 = tmp %>% ggplot(aes(x=Cloud3pm  )) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2)
p5 = tmp %>% ggplot(aes(x=Humidity9am  )) + geom_density(fill=mainPalette[5], colour=mainPalette[5], alpha = 0.2)
p6 = tmp %>% ggplot(aes(x=WindGustSpeed  )) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2)
p7 = tmp %>% ggplot(aes(x=Rainfall  )) + geom_density(fill=mainPalette[7], colour=mainPalette[7], alpha = 0.2) + scale_x_log10() 
p8 = tmp %>% ggplot(aes(x= WindGustDir, colour = WindGustDir, fill = WindGustDir  )) +
  geom_density(alpha = 0.2) + theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="none")

p9 = tmp %>% ggplot(aes(x=RainTomorrow  )) + geom_bar(fill=mainPalette[8], colour=mainPalette[8], alpha = 0.5)
grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9)

rm(p1,p2,p3,p4,p5,p6,p7,p8,p9,tmp)
```

### Missing Data

Further analysis of data shows that many features are missing. Some data losses are very significant. We are going to identify what data is missing and if it is feasible to recover the data.

```{r}
print(sort(colSums(is.na(weatherData)), decreasing = T))
```

 To speed up data processing and plot rendering we are going to use a data sample. For population of 142K observations, 20K sample size would be sufficient for 99% confidence level with the confidence interval 1.
```{r plot_aggr_missing, fig.align="center", fig.cap="Missing Data Summary"}
weatherSample = sample_n(weatherData, SampleSize)
aggr(weatherSample, numbers = F, prop = T, col = mainPalette, sortVars = T, bars = F, varheight = T)
``` 

As demonstrated in Figure \ref{fig:plot_aggr_missing} *Sunshine*, *Evaporation* and *Clouds* columns safer the loss of data between **48%** and **38%**. This is significant! Since we are dealing with the weather patterns we should be observing cyclical data patterns. Let's review data distribution of features that damaged the most.

```{r plot_margin1, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE,fig.align="center", fig.cap="Date/Evaporation Margin Plot", fig.height=4}
marginplot(weatherSample[, c("Date", "Evaporation")] , col = mainPalette, cex = 0.8, pch = 20)
```

```{r plot_margin2, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE,fig.align="center", fig.cap="Date/ Sunshine Margin Plot", fig.height=4}
marginplot(weatherSample[, c("Date", "Sunshine")] , col = mainPalette, pch = 20, cex = 0.8)
```

```{r plot_margin3, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Date/ Pressure3pm Margin Plot", fig.height=4}
marginplot(weatherSample[, c("Date", "Pressure9am")] , col = mainPalette, pch = 20, cex = 0.8)
```


So what do the margin plots tell us? First of all let's take a look at *Date* axis. The *Date* has been converted to number to ensure continuous flow of the data . All features we picked exhibit cyclical pattern as expected. Along the vertical axis we observe the box plot of the respective feature. *Evaporaton* data is quite remarkable (Figure  \ref{fig: plot_margin1}); it has very narrow distribution and a lot of so-called outliers. Though forces of nature follow seasonal patters they often exhibit wide range of seasonal anomalies, which the plots highlight. The distribution of the missing data of a given feature is depicted along the horizontal axis. In all three cases the missing data is randomly distributed along observed date range. Along the horizontal axis we may see box plots of the date and a given feature. *Presure9am* ((Figure  \ref{fig: plot_margin3})) distributed evenly across the observed date frame. *Evaporation* and *Sunshine* exhibit more data losses towards the end of the observed period  

Let's examine one more dimension of the missing data, namely features vs feature vs location


```{r plot_missLocation, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Missing Data By Location", fig.height= 9}
tmp = weatherData %>% group_by(Location) %>%
  select_if(function(x) any(is.na(x))) %>% 
  summarise_all(funs(sum(is.na(.))))  %>%  rowwise() %>% 
    summarize(Location, MissingObservations=sum(
  MinTemp + MaxTemp + Rainfall + Evaporation + Sunshine + WindGustDir + WindGustSpeed +
  WindDir9am + WindDir3pm + WindSpeed9am + WindSpeed3pm + Humidity9am + Humidity3pm + Pressure9am +
  Pressure3pm + Cloud9am + Cloud3pm + Temp9am + Temp3pm + RainToday))
ggplot(tmp, aes(x=Location, y=MissingObservations )) + 
  geom_bar(stat = "identity", fill=mainPalette[2], colour=mainPalette[2]) + coord_flip() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  

avgPerLocation = round(mean(tmp$MissingObservations), digits = 0)
rm(tmp)
```

Remarkably Figure \ref{fig:plot_missLocation} shows that **`r avgPerLocation`**  observations are missing on average per location. Though if we take a second look at the weather station map \ref{fig:map} we would see that Mount Gini (the station that miss the most data), Bendigo and Ballarat are close to Melbrun, where the staff has kept observing data on regular basis. Newcastle to Sydney and so on...

### Data correlation and other observations

Let's examine how the features are correlated to each other. Knowing weather we can make an accurate prediction that the temperature features should be highly correlated, as well as pressure, wind speed, clouds and humidity groups 
```{r plot_corr, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Data Correlation", out.width="1.1\\linewidth"}
corrplot(cor(select_if(weatherData, is.numeric), use="pairwise.complete.obs"),
         method="color", type="upper",order="hclust", col = mainPalette, number.cex = .6,
         addCoef.col = "black", tl.col="black", tl.srt=45,sig.level = 0.4, insig = "blank", diag=FALSE )
```

Figure \ref{fig:plot_corr} confirms our initial guess. This observation will help us to eliminate redundant features later when we get to the point of selecting useful predictors for our model

#### Takeaways from Data Exploration Excersize

* The data we are dealing with suffers major observation losses (Figure \ref{fig:plot_aggr_missing})
* The least represented features are 
+ *Sunshine* **48%** 
+ *Evaporation* **43%**
+ *Cloud* group (**40%** and **38%** respectively)
* The rest of the features exhibit medium to minor data losses, where *Pressure* group leads the way with 10% 
* The missing data is distributed randomly over the observed time frame (Figures \ref{fig:plot_margin1}, \ref{fig:plot_margin2}, \ref{fig:plot_margin3})
* We also witnessed that some weather stations recorded less data and some were almost prefect at record keeping (Figure \ref{fig:plot_missLocation}). Luckily many majority weather stations situate relatively close to each other (see figure \ref{fig:map}). Thus if a station has data gaps the neighboring station data could be used to approximate the missing data with plausible accuracy 
* We have also noticed that many features are either positively or negatively correlated (Figure \ref{fig:plot_corr}), where 
+ *MaxTemp*, *Temp3pm* and *Temp9am*  exhibits correlation of **0.86** to **0.98**
+ *Pressure9am* and *Pressure3pm* have correlation coefficient of **0.96**
+ *Sunshine* and *Cloud* group correlated negatively with coefficient of **-0.7**
+ *Rainfall* feature is of particular interest since this is what we are trying to predict. Unfortunately it does not demonstrate any strong correlations with any other feature
* Doing the data analysis we have also seen seasonal patterns and data that fall outside of the normal distribution range by far (outliers). Those are anomalies of nature.
* The last but not least the target feature (the value we are trying to predict) is unbalanced. so we are dealing with unbalanced data set. See Figure \ref{fig:feature_distribution} *RainTomorrow* plot

## Data Preparation

Data exploration confirmed that despite of significant data loss we should be able to impute data with high degree of plausibility

### Datas Imputing

Before we start dealing with missing observations let's do some feature engineering, which will 
+ improve imputation processing speed
+ improve model training performance and hopefully accuracy

First of all let's get rid of *Date* column. Outside of the presentation it does not carry too mach information. What would be useful indeed is a feature that captures seasonal observation fluctuations. That would bee *month* and *day* combined, giving us year-round (365) days of observations

Secondly we convert categorical features to numbers. But before we do so we would like to ponder about *Location*. We have couple options here. Either we convert the locations to the numbers or we can replace them with the real geographical coordinates. After some deliberation we can conclude that the coordinates will not add too much knowledge in the context of the model training. But they will certainly break this categorical feature (coordinates have 4,6 decimal places, which effectively make them continuous). So we stick with categories.

This is our original set:
```{r}
str(weatherData)
```

Transformation
```{r}
data = mutate(weatherData,MMDD = as.numeric( format(Date, "%m%d")),Location = unclass(Location), 
              WindGustDir = unclass(WindGustDir),
              WindDir9am = unclass(WindDir9am), WindDir3pm = unclass(WindDir3pm),
              RainToday = unclass(RainToday)-1, RainTomorrow = unclass(RainTomorrow)-1)
data =  subset(data, select = -Date)
```
Resulting data frame structure:
```{r}
str(data)
```

To impute the missing data we employ **MICE** package. Our imputation strategy is to employ **Predictive mean matching** model which is a robust, fast imputation algorithm that works with numeric values ( this is why we have converted all data to the numeric values)
Lets do a dry run first to see what predictors and methods for each feature to cure *MICE* software chooses. As before we will be working with a 20K data sample. Imputation process on the whole set take about 3 hours and 20 minutes to complete!
In addition we let *MICE* to choose predictors for us running **quickpred()** method

```{r}
meta = mice(data, maxit = 0, print = FALSE)
weatherSample = sample_n(data, SampleSize)
methods = meta$method
predictors = quickpred(data) 
```

Let's review the methods chosen by the software making sure that they meet our requirements highlighted prior in the imputation strategy paragraph
```{r}
print(methods)
```
The code output above shows that 
1 the features without missing data will not be imputed
2 The imputation targets will all be treated with *Predictive mean matching* algorithm ("pmm")

This is exactly what we need. Now let's review the predictors (*The command output is not included into report to save space *)
```{r include=FALSE}
print(predictors)
```
The matrix of predictors has the predictors in the columns and the features to be imputed in the rows. If the cell value equals has **1** the predictor will be employed in calculations for the respective imputation target. Surprisingly **MMDD** is not used widely to predict the missing data, nether do the **Location**. 

Now  we are going to start the imputation process. **Note: it might take about 4 - 5 minutes even for a smaple**. We have disabled the output of the function as we do not want to pollute the report with irrelevant messages
```{r}
imputed = mice(weatherSample, pred = predictors, meth = methods, seed = 38019,
               nnet.MaxNWts = 2000, printFlag = F)
```
Now it is time to analyze the imputed values. In general, a good imputed value is a value that could have been observed had it not been missing. The MAR assumption can never be tested from the observed data. To check whether the imputations created by **MICE** algorithm are plausible we employ density  charts and compare the distribution of the imputed values vs real observations. Let's do this (*again the plots take time, patience...*).

```{r imputed_density,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Imputed Values Distribution vs Real Observations", out.width="1.1\\linewidth"}
densityplot(x = imputed,data = ~MinTemp + MaxTemp + Rainfall + WindGustDir + WindGustSpeed + 
            WindDir9am + WindDir3pm + WindSpeed9am + WindSpeed3pm + Humidity9am + Humidity3pm +  
            Pressure9am + Pressure3pm + Temp9am + Temp3pm + RainToday,plot.points = F, col = mainPalette)

```

Figure \ref{fig:imputed_density} illustrates imputed value distribution for each imputed feature vs observed data. The fat green line renders the real data distribution and the thin lines of the other colors the distribution of imputed values after each imputation cycle (*there are five of them by default*). Where the last one is yellow. The yellow line should be shadowing the contour of the green one as close as possible, which give us an indication that the result of the imputation is plausible. Looking at the charts we can conclude that the imputation has been successful! Let's apply imputed values to our sample set and verify if there are any *NAs* left
```{r}
weatherSample = complete(imputed)
print(colSums(is.na(weatherSample)))
```
Outstanding! There are no missing values. Now we move on to the next part - model training
```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# clean the memory
rm(methods,meta,imputed,predictors,data)
```

# Modeling and Evalutation

Finally we have reached the stage where we can start training and evaluating classification models. At this point we have clear understanding of our data. We have gotten rid of the features that did not present much value. We have filled the gaps in our data set employing sophisticated imputation technique.

## Feature Selection

The weather observation data set originally had 24 features. We have removed *RISK_MM* and *Date* as explained earlier and added *MMDD*. Now the data set has 22 features and one label. Let's see if we can reduce the number of predictors without significant information loss. This would make our models faster and more interpretable for users. We shall keep in mind that at the data exploration phase we have discovered that many features are correlated (Figure \ref{fig:plot_corr}). hopefully this knowledge will help us identify and remove redundant features.

Generally speaking feature evaluation methods can be separated into two groups: those that use the model information and those that do not. Clearly at this stage the models are not ready. Thus we will be exploring the methods that do not require model.

This group of the method could be spit further as follows:

* wrapper methods that evaluate multiple models adding and/or removing predictors. These are some examples:
+ recursive feature elimination
+ genetic algorithms
+ simulated annealing

* filter methods which evaluate the relevance of the predictors outside of the predictive models. 

The evaluation of various feature selection methods is not in the scope of this paper. Thus we opt for a recursive feature elimination method using accuracy as a target metric. 

Before we precede any further let's ensure that all categorical values get converted to factors. This is useful for dimentiality reduction algorithms and model training.  

```{r}
weatherSample = mutate(weatherSample, Location = as.factor(unclass(Location)), WindGustDir = as.factor(unclass(WindGustDir)),
              WindDir9am = as.factor(unclass(WindDir9am)), WindDir3pm = as.factor(unclass(WindDir3pm)),
              RainToday = as.factor(unclass(RainToday)), RainTomorrow = as.factor(unclass(RainTomorrow)))
```

Let's run feature selection algorithm
```{r}
predictors = subset(weatherSample,select = -RainTomorrow)
label = weatherSample[,22]

# run the RFE algorithm
rfePrediction = rfe(predictors, label, sizes=c(1:22), 
                    rfeControl = rfeControl(functions=rfFuncs, method="cv", number=3))
print(rfePrediction)
```

```{r plot_feature_selection,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Number of Predictors vs Accuracy", out.width="1.1\\linewidth"}
plot(rfePrediction, type=c("g", "o"))
```
Figure \ref{fig:plot_feature_selection} shows that accuracy peaks a few times:  around 9 predictors, then around 14 and tops at 22.The accuracy gain between 14 and 22 is negligible. Here is the list of features ordered by importance:
```{r}
print(predictors(rfePrediction))
```
```{r include=FALSE}
# remove useless variables
rm(label,predictors,avgPerLocation,rfePrediction)
```
### Data Upsampling

There is one more step before we get to the model training. As shown in Figure \ref{fig:feature_distribution} our data set is unbalanced. This could cause model overfiting. So let's split the data into the training and testing sets and up-sample the training set
```{r}
set.seed(1608)
splitIdx = createDataPartition(weatherSample$RainTomorrow, p=0.7, list = F)  # 70% training data
trainData = weatherSample[splitIdx, ]
testData = weatherSample[-splitIdx, ]

set.seed(590045)
columns = colnames(trainData)
trainData = upSample(x = trainData[, columns[columns != "RainTomorrow"] ], y = trainData$RainTomorrow, list = F, yname = "RainTomorrow")
# verify data balance in training balanced set
print(table(trainData$RainTomorrow))
```
As we can see the training set is balanced.

Thus we have prepared our training and test data sets. We have identified the most important features. We are ready to work on the prediction models

## Decision Tree Model

## Naive Bayes Model

## Random Forest Model

## Logistic Regression Model

## Model Comparison


# Model Deployment

# Conclusion

# Bibliography
