% !TeX root = RJwrapper.tex
\title{Rain in Australia. Classification Prediction Model}
\author{by Sumaira Afzal, Viraja Ketkar, Murlidhar Loka, Vadim Spirkov}

\maketitle

\abstract{%
Many native cultures comprise an institution of ``rainmakers'' -- people
who would not as much invoke the rains, but anticipate them based on
ethno-meteorology. The forecasting was based on skillful art of
observing the natural environment as expressed in the timing or
flowering of plants, hatching of insects, arrival of migratory birds,
etc., which enables farmers to make adjustments in farming calendar and
crop selection types in any given season. This indigenous knowledge was
often passed down from one generation to the other. We are going to
employ the latest scientific methods, prediction algorithms to achieve
the same very goal without thorough knowledge of forces of nature,
hopefully with the same accuracy as the aboriginal people
}

% Any extra LaTeX you need in the preamble

\hypertarget{background}{%
\subsection{Background}\label{background}}

Weather forecasting is a complex and often challenging skill that
involves observing and processing vast amounts of data. Weather systems
can range from small, short lived thunderstorms only a few kilometers in
diameter that last a couple hours to large scale rain and snow storms up
to a thousand kilometers in diameter and lasting for days.

A very important component of modern weather forecasting is the use of
numerical weather prediction (NWP) models. In the last years, the
forecast quality of those models constantly improved, mostly due to
major improvements in high performance computing. NWP focuses on taking
current observations of weather and processing these data with computer
models to forecast the future state of weather. Knowing the current
state of the weather is just as important as the numerical computer
models processing the data. Current weather observations serve as input
to the numerical computer models through a process known as data
assimilation to produce outputs of temperature, precipitation, and
hundreds of other meteorological elements from the oceans to the top of
the atmosphere.

\hypertarget{objective}{%
\subsection{Objective}\label{objective}}

The objective of this research is to find a numerical weather prediction
model that would provide accurate forecast of possibility of the rain
next day having today weather pattern observations. In addition to
accuracy the model should be easily interpretable and flexible enough to
accept limited number of input features without diminishing its
prediction power.

\hypertarget{data-analisys}{%
\section{Data Analisys}\label{data-analisys}}

The data set we are going to use for our research contains daily weather
observations from numerous Australian weather stations from 2007 till
2017. There are over 142000 records. It has been sourced from
\href{https://www.kaggle.com/jsphyg/weather-dataset-rattle-package}{Kaggle}

\hypertarget{data-dictionary}{%
\subsection{Data Dictionary}\label{data-dictionary}}

We exclude the variable Risk-MM when training your binary classification
model. If we don't exclude it, you will leak the answers to our model
and reduce its predictability

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.52\columnwidth}\raggedright
Column Name\strut
\end{minipage} & \begin{minipage}[b]{0.43\columnwidth}\raggedright
Column Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Date\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Date of observation\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Location\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Common name of the location of the weather station\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
MinTemp\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Minimum temperature in degrees Celsius\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
MaxTemp\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Maximum temperature in degrees Celsius\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Rainfall\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Amount of rainfall recorded for the day in mm\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Evaporation\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
So-called Class A pan evaporation (mm) in the 24 hours to 9am\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Sunshine\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Number of hours of bright sunshine in the day\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
WindGustDir\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Direction of the strongest wind gust in the 24 hours to midnight\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
WindGustSpeed\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Speed (km/h) of the strongest wind gust in the 24 hours to
midnight\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
WindDir9amDirection\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Of the wind at 9am\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
WindDir3pmDirection\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Of the wind at 3pm\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
WindSpeed9amWind\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Wind speed (km/hr) averaged over 10 minutes prior to 9am\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
WindSpeed3pmWind\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Wind Speed (km/hr) averaged over 10 minutes prior to 3pm\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Humidity9amHumidity\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Humidity (percent) at 9am\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Humidity3pmHumidity\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Humidity (percent) at 3pm\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Pressure9amAtmospheric\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Pressure (hpa) reduced to mean sea level at 9am\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Pressure3pmAtmospheric\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Pressure (hpa) reduced to mean sea level at 3pm\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Cloud9amFraction\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Area of sky obscured by cloud at 9am. This is measured in ``oktas'',
which are a unit of eights. It records how many eights of the sky are
obscured by cloud. A 0 measure indicates completely clear sky whilst an
8 indicates that it is completely overcast\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Cloud3pmFraction\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Area of sky obscured by cloud (in ``oktas'': eighths) at 3pm. See
Cloud9am for a description of the values\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Temp9amTemperature\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Temperature (degrees C) at 9am\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
Temp3pmTemperature\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Temperature (degrees C) at 3pm\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
RainTodayBoolean\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Rainy today. 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm,
otherwise 0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
RISK\_MM\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
Amount of rain. A kind of measure of the ``risk''. This column is
redundant and will be dropped\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\raggedright
\textbf{RainTomorrowThe}\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
\textbf{Target variable. Will it rain tomorrow?}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{data-exploration}{%
\subsection{Data Exploration}\label{data-exploration}}

Let's take a close look at the data set. We start with loading weather
observations from the file into a data frame. We remove RISK\_MM as
explained and convert Date column to \emph{date} format

\begin{Schunk}
\begin{Sinput}
weatherData = read.csv("../data/weatherAUS.csv", header = TRUE, na.strings = c("NA","","#NA"),sep=",")
weatherData = subset(weatherData, select = -RISK_MM)
weatherData$Date = as.Date(as.character(weatherData$Date),"%Y-%m-%d")
\end{Sinput}
\end{Schunk}

Now let's load coordinates of the weather stations and have a bird-eye
view of the weather station locations

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics[width=1.1\linewidth]{images/weatherStations} 

}

\caption[Australian Weather Stations]{Australian Weather Stations}\label{fig:map}
\end{figure}
\end{Schunk}

Let's review data summary

Data sample

Next set of plots renders distribution of a few selected features.

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics{main_files/figure-latex/feature_distribution-1} 

}

\caption[Observations Distribution]{Observations Distribution}\label{fig:feature_distribution}
\end{figure}
\end{Schunk}

\hypertarget{missing-data}{%
\subsubsection{Missing Data}\label{missing-data}}

Further analysis of data shows that many features are missing. Some data
losses are very significant. We are going to identify what data is
missing and if it is feasible to recover the data.

\begin{Schunk}
\begin{Sinput}
print(sort(colSums(is.na(weatherData)), decreasing = T))
\end{Sinput}
\begin{Soutput}
#>      Sunshine   Evaporation      Cloud3pm      Cloud9am   Pressure9am 
#>         67816         60843         57094         53657         14014 
#>   Pressure3pm    WindDir9am   WindGustDir WindGustSpeed    WindDir3pm 
#>         13981         10013          9330          9270          3778 
#>   Humidity3pm       Temp3pm  WindSpeed3pm   Humidity9am      Rainfall 
#>          3610          2726          2630          1774          1406 
#>     RainToday  WindSpeed9am       Temp9am       MinTemp       MaxTemp 
#>          1406          1348           904           637           322 
#>          Date      Location  RainTomorrow 
#>             0             0             0
\end{Soutput}
\end{Schunk}

To speed up data processing and plot rendering we are going to use a
data sample. For population of 142K observations, 20K sample size would
be sufficient for 99\% confidence level with the confidence interval 1.

\begin{Schunk}
\begin{Sinput}
weatherSample = sample_n(weatherData, SampleSize)
aggr(weatherSample, numbers = F, prop = T, col = mainPalette, sortVars = T, bars = F, varheight = T)
\end{Sinput}
\begin{figure}[H]

{\centering \includegraphics{main_files/figure-latex/plot_aggr_missing-1} 

}

\caption[Missing Data Summary]{Missing Data Summary}\label{fig:plot_aggr_missing}
\end{figure}
\begin{Soutput}
#> 
#>  Variables sorted by number of missings: 
#>       Variable  Count
#>       Sunshine 0.4760
#>    Evaporation 0.4326
#>       Cloud3pm 0.4038
#>       Cloud9am 0.3843
#>    Pressure3pm 0.0946
#>    Pressure9am 0.0945
#>     WindDir9am 0.0694
#>    WindGustDir 0.0682
#>  WindGustSpeed 0.0678
#>     WindDir3pm 0.0254
#>    Humidity3pm 0.0237
#>        Temp3pm 0.0173
#>   WindSpeed3pm 0.0167
#>    Humidity9am 0.0133
#>       Rainfall 0.0099
#>   WindSpeed9am 0.0099
#>      RainToday 0.0099
#>        Temp9am 0.0069
#>        MinTemp 0.0048
#>        MaxTemp 0.0022
#>           Date 0.0000
#>       Location 0.0000
#>   RainTomorrow 0.0000
\end{Soutput}
\end{Schunk}

As demonstrated in Figure \ref{fig:plot_aggr_missing} \emph{Sunshine},
\emph{Evaporation} and \emph{Clouds} columns safer the loss of data
between \textbf{48\%} and \textbf{38\%}. This is significant! Since we
are dealing with the weather patterns we should be observing cyclical
data patterns. Let's review data distribution of features that damaged
the most.

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics{main_files/figure-latex/plot_margin1-1} 

}

\caption[Date/Evaporation Margin Plot]{Date/Evaporation Margin Plot}\label{fig:plot_margin1}
\end{figure}
\end{Schunk}

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics{main_files/figure-latex/plot_margin2-1} 

}

\caption[Date/ Sunshine Margin Plot]{Date/ Sunshine Margin Plot}\label{fig:plot_margin2}
\end{figure}
\end{Schunk}

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics{main_files/figure-latex/plot_margin3-1} 

}

\caption[Date/ Pressure3pm Margin Plot]{Date/ Pressure3pm Margin Plot}\label{fig:plot_margin3}
\end{figure}
\end{Schunk}

So what do the margin plots tell us? First of all let's take a look at
\emph{Date} axis. The \emph{Date} has been converted to number to ensure
continuous flow of the data . All features we picked exhibit cyclical
pattern as expected. Along the vertical axis we observe the box plot of
the respective feature. \emph{Evaporaton} data is quite remarkable
(Figure \ref{fig:plot_margin1}); it has very narrow distribution and a
lot of so-called outliers. Though forces of nature follow seasonal
patters they often exhibit wide range of seasonal anomalies, which the
plots highlight. The distribution of the missing data of a given feature
is depicted along the horizontal axis. In all three cases the missing
data is randomly distributed along observed date range. Along the
horizontal axis we may see box plots of the date and a given feature.
\emph{Presure9am} ((Figure \ref{fig:plot_margin3})) distributed evenly
across the observed date frame. \emph{Evaporation} and \emph{Sunshine}
exhibit more data losses towards the end of the observed period

Let's examine one more dimension of the missing data, namely features vs
feature vs location

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics{main_files/figure-latex/plot_missLocation-1} 

}

\caption[Missing Data By Location]{Missing Data By Location}\label{fig:plot_missLocation}
\end{figure}
\end{Schunk}

Remarkably Figure \ref{fig:plot_missLocation} shows that \textbf{6460}
observations are missing on average per location. Though if we take a
second look at the weather station map \ref{fig:map} we would see that
Mount Gini (the station that miss the most data), Bendigo and Ballarat
are close to Melbrun, where the staff has kept observing data on regular
basis. Newcastle to Sydney and so on\ldots{}

\hypertarget{data-correlation-and-other-observations}{%
\subsubsection{Data correlation and other
observations}\label{data-correlation-and-other-observations}}

Let's examine how the features are correlated to each other. Knowing
weather we can make an accurate prediction that the temperature features
should be highly correlated, as well as pressure, wind speed, clouds and
humidity groups

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics[width=1.1\linewidth]{main_files/figure-latex/plot_corr-1} 

}

\caption[Data Correlation]{Data Correlation}\label{fig:plot_corr}
\end{figure}
\end{Schunk}

Figure \ref{fig:plot_corr} confirms our initial guess. This observation
will help us to eliminate redundant features later when we get to the
point of selecting useful predictors for our model

\hypertarget{takeaways-from-data-exploration-excersize}{%
\paragraph{Takeaways from Data Exploration
Excersize}\label{takeaways-from-data-exploration-excersize}}

\begin{itemize}
\tightlist
\item
  The data we are dealing with suffers major observation losses (Figure
  \ref{fig:plot_aggr_missing})
\item
  The least represented features are
\item
  \emph{Sunshine} \textbf{48\%}
\item
  \emph{Evaporation} \textbf{43\%}
\item
  \emph{Cloud} group (\textbf{40\%} and \textbf{38\%} respectively)
\item
  The rest of the features exhibit medium to minor data losses, where
  \emph{Pressure} group leads the way with 10\%
\item
  The missing data is distributed randomly over the observed time frame
  (Figures \ref{fig:plot_margin1}, \ref{fig:plot_margin2},
  \ref{fig:plot_margin3})
\item
  We also witnessed that some weather stations recorded less data and
  some were almost prefect at record keeping (Figure
  \ref{fig:plot_missLocation}). Luckily many majority weather stations
  situate relatively close to each other (see figure \ref{fig:map}).
  Thus if a station has data gaps the neighboring station data could be
  used to approximate the missing data with plausible accuracy
\item
  We have also noticed that many features are either positively or
  negatively correlated (Figure \ref{fig:plot_corr}), where
\item
  \emph{MaxTemp}, \emph{Temp3pm} and \emph{Temp9am} exhibits correlation
  of \textbf{0.86} to \textbf{0.98}
\item
  \emph{Pressure9am} and \emph{Pressure3pm} have correlation coefficient
  of \textbf{0.96}
\item
  \emph{Sunshine} and \emph{Cloud} group correlated negatively with
  coefficient of \textbf{-0.7}
\item
  \emph{Rainfall} feature is of particular interest since this is what
  we are trying to predict. Unfortunately it does not demonstrate any
  strong correlations with any other feature
\item
  Doing the data analysis we have also seen seasonal patterns and data
  that fall outside of the normal distribution range by far (outliers).
  Those are anomalies of nature.
\item
  The last but not least the target feature (the value we are trying to
  predict) is unbalanced. so we are dealing with unbalanced data set.
  See Figure \ref{fig:feature_distribution} \emph{RainTomorrow} plot
\end{itemize}

\hypertarget{data-preparation}{%
\subsection{Data Preparation}\label{data-preparation}}

Data exploration confirmed that despite of significant data loss we
should be able to impute data with high degree of plausibility

\hypertarget{datas-imputing}{%
\subsubsection{Datas Imputing}\label{datas-imputing}}

Before we start dealing with missing observations let's do some feature
engineering, which will + improve imputation processing speed + improve
model training performance and hopefully accuracy

First of all let's get rid of \emph{Date} column. Outside of the
presentation it does not carry too mach information. What would be
useful indeed is a feature that captures seasonal observation
fluctuations. That would bee \emph{month} and \emph{day} combined,
giving us year-round (365) days of observations

Secondly we convert categorical features to numbers. But before we do so
we would like to ponder about \emph{Location}. We have couple options
here. Either we convert the locations to the numbers or we can replace
them with the real geographical coordinates. After some deliberation we
can conclude that the coordinates will not add too much knowledge in the
context of the model training. But they will certainly break this
categorical feature (coordinates have 4,6 decimal places, which
effectively make them continuous). So we stick with categories.

This is our original set:

Transformation

\begin{Schunk}
\begin{Sinput}
data = mutate(weatherData,MMDD = as.numeric( format(Date, "%m%d")),Location = unclass(Location),   
              WindGustDir = unclass(WindGustDir), 
              WindDir9am = unclass(WindDir9am), WindDir3pm = unclass(WindDir3pm),
              RainToday = unclass(RainToday)-1, RainTomorrow = unclass(RainTomorrow)-1)
data =  subset(data, select = -Date)
\end{Sinput}
\end{Schunk}

Resulting data frame structure:

To impute the missing data we employ \textbf{MICE} package. Our
imputation strategy is to employ \textbf{Predictive mean matching} model
which is a robust, fast imputation algorithm that works with numeric
values ( this is why we have converted all data to the numeric values)
Lets do a dry run first to see what predictors and methods for each
feature to cure \emph{MICE} software chooses. As before we will be
working with a 20K data sample. Imputation process on the whole set take
about 3 hours and 20 minutes to complete! In addition we let \emph{MICE}
to choose predictors for us running \textbf{quickpred()} method

\begin{Schunk}
\begin{Sinput}
meta = mice(data, maxit = 0, print = FALSE)
weatherSample = sample_n(data, SampleSize)
methods = meta$method
predictors = quickpred(data) 
\end{Sinput}
\end{Schunk}

Let's review the methods chosen by the software making sure that they
meet our requirements highlighted prior in the imputation strategy
paragraph

The code output above shows that 1 the features without missing data
will not be imputed 2 The imputation targets will all be treated with
\emph{Predictive mean matching} algorithm (``pmm'')

This is exactly what we need. Now let's review the predictors (\emph{The
command output is not included into report to save space })

The matrix of predictors has the predictors in the columns and the
features to be imputed in the rows. If the cell value equals has
\textbf{1} the predictor will be employed in calculations for the
respective imputation target. Surprisingly \textbf{MMDD} is not used
widely to predict the missing data, nether do the \textbf{Location}.

Now we are going to start the imputation process. \textbf{Note: it might
take about 4 - 5 minutes even for a smaple}. We have disabled the output
of the function as we do not want to pollute the report with irrelevant
messages

\begin{Schunk}
\begin{Sinput}
imputed = mice(weatherSample, pred = predictors, meth = methods, seed = 38019,
               nnet.MaxNWts = 2000, printFlag = F)
\end{Sinput}
\end{Schunk}

Now it is time to analyze the imputed values. In general, a good imputed
value is a value that could have been observed had it not been missing.
The MAR assumption can never be tested from the observed data. To check
whether the imputations created by \textbf{MICE} algorithm are plausible
we employ density charts and compare the distribution of the imputed
values vs real observations. Let's do this (\emph{again the plots take
time, patience\ldots{}}).

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics[width=1.1\linewidth]{main_files/figure-latex/imputed_density-1} 

}

\caption[Imputed Values Distribution vs Real Observations]{Imputed Values Distribution vs Real Observations}\label{fig:imputed_density}
\end{figure}
\end{Schunk}

Figure \ref{fig:imputed_density} illustrates imputed value distribution
for each imputed feature vs observed data. The fat green line renders
the real data distribution and the thin lines of the other colors the
distribution of imputed values after each imputation cycle (\emph{there
are five of them by default}). Where the last one is yellow. The yellow
line should be shadowing the contour of the green one as close as
possible, which give us an indication that the result of the imputation
is plausible. Looking at the charts we can conclude that the imputation
has been successful! Let's apply imputed values to our sample set and
verify if there are any \emph{NAs} left

\begin{Schunk}
\begin{Sinput}
weatherSample = complete(imputed)
print(colSums(is.na(weatherSample)))
\end{Sinput}
\begin{Soutput}
#>      Location       MinTemp       MaxTemp      Rainfall   Evaporation 
#>             0             0             0             0             0 
#>      Sunshine   WindGustDir WindGustSpeed    WindDir9am    WindDir3pm 
#>             0             0             0             0             0 
#>  WindSpeed9am  WindSpeed3pm   Humidity9am   Humidity3pm   Pressure9am 
#>             0             0             0             0             0 
#>   Pressure3pm      Cloud9am      Cloud3pm       Temp9am       Temp3pm 
#>             0             0             0             0             0 
#>     RainToday  RainTomorrow          MMDD 
#>             0             0             0
\end{Soutput}
\end{Schunk}

Outstanding! There are no missing values. Now we move on to the next
part - model training

\hypertarget{modeling-and-evalutation}{%
\section{Modeling and Evalutation}\label{modeling-and-evalutation}}

Finally we have reached the stage where we can start training and
evaluating classification models. At this point we have clear
understanding of our data. We have gotten rid of the features that did
not present much value. We have filled the gaps in our data set
employing sophisticated imputation technique.

\hypertarget{feature-selection}{%
\subsection{Feature Selection}\label{feature-selection}}

The weather observation data set originally had 24 features. We have
removed \emph{RISK\_MM} and \emph{Date} as explained earlier and added
\emph{MMDD}. Now the data set has 22 features and one label. Let's see
if we can reduce the number of predictors without significant
information loss. This would make our models faster and more
interpretable for users. We shall keep in mind that at the data
exploration phase we have discovered that many features are correlated
(Figure \ref{fig:plot_corr}). hopefully this knowledge will help us
identify and remove redundant features.

Generally speaking feature evaluation methods can be separated into two
groups: those that use the model information and those that do not.
Clearly at this stage the models are not ready. Thus we will be
exploring the methods that do not require model.

This group of the method could be spit further as follows:

\begin{itemize}
\item
  wrapper methods that evaluate multiple models adding and/or removing
  predictors. These are some examples:
\item
  recursive feature elimination
\item
  genetic algorithms
\item
  simulated annealing
\item
  filter methods which evaluate the relevance of the predictors outside
  of the predictive models.
\end{itemize}

The evaluation of various feature selection methods is not in the scope
of this paper. Thus we opt for a recursive feature elimination method
using accuracy as a target metric.

Before we precede any further let's ensure that all categorical values
get converted to factors. This is useful for dimentiality reduction
algorithms and model training.

\begin{Schunk}
\begin{Sinput}
weatherSample = mutate(weatherSample, Location = as.factor(unclass(Location)), 
          WindGustDir = as.factor(unclass(WindGustDir)),
          WindDir9am = as.factor(unclass(WindDir9am)), WindDir3pm = as.factor(unclass(WindDir3pm)),
          RainToday = as.factor(unclass(RainToday)), RainTomorrow = as.factor(unclass(RainTomorrow)))
\end{Sinput}
\end{Schunk}

Let's run feature selection algorithm

\begin{Schunk}
\begin{Sinput}
predictors = subset(weatherSample,select = -RainTomorrow)
label = weatherSample[,22]

# run the RFE algorithm
rfePrediction = rfe(predictors, label, sizes=c(1:22), 
                    rfeControl = rfeControl(functions=rfFuncs, method="cv", number=3))
print(rfePrediction)
\end{Sinput}
\begin{Soutput}
#> 
#> Recursive feature selection
#> 
#> Outer resampling method: Cross-Validated (3 fold) 
#> 
#> Resampling performance over subset size:
#> 
#>  Variables Accuracy  Kappa AccuracySD KappaSD Selected
#>          1   0.8231 0.3732   0.002468 0.01553         
#>          2   0.8146 0.3843   0.005100 0.02274         
#>          3   0.8283 0.4546   0.007749 0.02330         
#>          4   0.8302 0.4752   0.012749 0.04676         
#>          5   0.8327 0.4899   0.007866 0.02774         
#>          6   0.8407 0.5218   0.007640 0.02944         
#>          7   0.8467 0.5377   0.003886 0.02030         
#>          8   0.8490 0.5438   0.006280 0.02311         
#>          9   0.8476 0.5377   0.005532 0.02489         
#>         10   0.8476 0.5376   0.004424 0.02519         
#>         11   0.8486 0.5409   0.004924 0.02723         
#>         12   0.8519 0.5514   0.002697 0.01732        *
#>         13   0.8495 0.5461   0.001724 0.02027         
#>         14   0.8486 0.5445   0.003151 0.02161         
#>         15   0.8484 0.5458   0.002949 0.01751         
#>         16   0.8496 0.5484   0.002086 0.01604         
#>         17   0.8509 0.5497   0.002641 0.01616         
#>         18   0.8511 0.5518   0.001401 0.01339         
#>         19   0.8507 0.5504   0.002615 0.01296         
#>         20   0.8514 0.5503   0.001546 0.01198         
#>         21   0.8516 0.5502   0.003429 0.01777         
#>         22   0.8512 0.5488   0.003117 0.01604         
#> 
#> The top 5 variables (out of 12):
#>    Humidity3pm, Sunshine, WindGustSpeed, Location, Cloud3pm
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics[width=1.1\linewidth]{main_files/figure-latex/plot_feature_selection-1} 

}

\caption[Number of Predictors vs Accuracy]{Number of Predictors vs Accuracy}\label{fig:plot_feature_selection}
\end{figure}
\end{Schunk}

Figure \ref{fig:plot_feature_selection} shows that accuracy peaks a few
times: with 9 predictors, 14 and tops at 22.The accuracy gain between 9
and 22 is negligible. Here is the list of features ordered by
importance. We take first nine for model training.

\hypertarget{data-upsampling}{%
\subsubsection{Data Upsampling}\label{data-upsampling}}

There is one more step before we get to the model training. As shown in
Figure \ref{fig:feature_distribution} our data set is unbalanced. This
could cause model over-fitting. So let's split the data into the
training and testing sets and up-sample the training set

\begin{Schunk}
\begin{Soutput}
#> 
#>    0    1 
#> 5432 5432
\end{Soutput}
\end{Schunk}

As we can see the training set is balanced.

Thus we have prepared our training and test data sets. We have
identified the most important features. We are ready to work on the
prediction models

\hypertarget{classification-decision-tree-model}{%
\subsection{Classification (Decision) Tree
Model}\label{classification-decision-tree-model}}

\begin{Schunk}
\begin{Soutput}
#> Conditional Inference Tree 
#> 
#> 10864 samples
#>     9 predictor
#>     2 classes: 'no', 'yes' 
#> 
#> No pre-processing
#> Resampling: Cross-Validated (5 fold) 
#> Summary of sample sizes: 8692, 8690, 8691, 8692, 8691 
#> Resampling results across tuning parameters:
#> 
#>   mincriterion  ROC        Sens       Spec     
#>   0.01          0.8929945  0.7916029  0.8317392
#>   0.50          0.8890711  0.7816611  0.8337681
#>   0.99          0.8782693  0.7814832  0.8022821
#> 
#> ROC was used to select the optimal model using the largest value.
#> The final value used for the model was mincriterion = 0.01.
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
confusionMatrix(data = pred.classTreeModel.raw, testDataCopy$RainTomorrow)
\end{Sinput}
\begin{Soutput}
#> Confusion Matrix and Statistics
#> 
#>           Reference
#> Prediction   no  yes
#>        no  1837  187
#>        yes  490  485
#>                                           
#>                Accuracy : 0.7743          
#>                  95% CI : (0.7589, 0.7891)
#>     No Information Rate : 0.7759          
#>     P-Value [Acc > NIR] : 0.5966          
#>                                           
#>                   Kappa : 0.4405          
#>  Mcnemar's Test P-Value : <2e-16          
#>                                           
#>             Sensitivity : 0.7894          
#>             Specificity : 0.7217          
#>          Pos Pred Value : 0.9076          
#>          Neg Pred Value : 0.4974          
#>              Prevalence : 0.7759          
#>          Detection Rate : 0.6125          
#>    Detection Prevalence : 0.6749          
#>       Balanced Accuracy : 0.7556          
#>                                           
#>        'Positive' Class : no              
#> 
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics[width=1.1\linewidth]{main_files/figure-latex/plot_classTree_ROC-1} 

}

\caption[Classification Tree Model AUC and ROC Curve]{Classification Tree Model AUC and ROC Curve}\label{fig:plot_classTree_ROC}
\end{figure}
\end{Schunk}

\hypertarget{naive-bayes-model}{%
\subsection{Naive Bayes Model}\label{naive-bayes-model}}

\begin{Schunk}
\begin{Soutput}
#> Naive Bayes 
#> 
#> 10864 samples
#>     9 predictor
#>     2 classes: 'no', 'yes' 
#> 
#> No pre-processing
#> Resampling: Cross-Validated (5 fold) 
#> Summary of sample sizes: 8692, 8690, 8691, 8692, 8691 
#> Resampling results across tuning parameters:
#> 
#>   usekernel  ROC        Sens       Spec     
#>   FALSE      0.7469454  0.5716072  0.7711680
#>    TRUE      0.8669656  0.6923751  0.8562226
#> 
#> Tuning parameter 'fL' was held constant at a value of 0
#> Tuning
#>  parameter 'adjust' was held constant at a value of 1
#> ROC was used to select the optimal model using the largest value.
#> The final values used for the model were fL = 0, usekernel = TRUE
#>  and adjust = 1.
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
confusionMatrix(data = pred.naiveBayesModel.raw, testDataCopy$RainTomorrow)
\end{Sinput}
\begin{Soutput}
#> Confusion Matrix and Statistics
#> 
#>           Reference
#> Prediction   no  yes
#>        no  1624  128
#>        yes  703  544
#>                                           
#>                Accuracy : 0.7229          
#>                  95% CI : (0.7065, 0.7389)
#>     No Information Rate : 0.7759          
#>     P-Value [Acc > NIR] : 1               
#>                                           
#>                   Kappa : 0.389           
#>  Mcnemar's Test P-Value : <2e-16          
#>                                           
#>             Sensitivity : 0.6979          
#>             Specificity : 0.8095          
#>          Pos Pred Value : 0.9269          
#>          Neg Pred Value : 0.4362          
#>              Prevalence : 0.7759          
#>          Detection Rate : 0.5415          
#>    Detection Prevalence : 0.5842          
#>       Balanced Accuracy : 0.7537          
#>                                           
#>        'Positive' Class : no              
#> 
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics[width=1.1\linewidth]{main_files/figure-latex/plot_nb_ROC-1} 

}

\caption[Naive Bayes Model AUC and ROC Curve]{Naive Bayes Model AUC and ROC Curve}\label{fig:plot_nb_ROC}
\end{figure}
\end{Schunk}

\hypertarget{random-forest-model}{%
\subsection{Random Forest Model}\label{random-forest-model}}

\begin{Schunk}
\begin{Soutput}
#> Random Forest 
#> 
#> 10864 samples
#>     9 predictor
#>     2 classes: 'no', 'yes' 
#> 
#> No pre-processing
#> Resampling: Cross-Validated (2 fold) 
#> Summary of sample sizes: 5432, 5432 
#> Resampling results across tuning parameters:
#> 
#>   mtry  ROC        Sens       Spec     
#>    2    0.8972216  0.7873711  0.8438881
#>   29    0.9664085  0.8630339  0.9401694
#>   56    0.9638440  0.8599043  0.9401694
#> 
#> ROC was used to select the optimal model using the largest value.
#> The final value used for the model was mtry = 29.
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
confusionMatrix(data = pred.randomForestModel.raw, testDataCopy$RainTomorrow)
\end{Sinput}
\begin{Soutput}
#> Confusion Matrix and Statistics
#> 
#>           Reference
#> Prediction   no  yes
#>        no  2093  274
#>        yes  234  398
#>                                           
#>                Accuracy : 0.8306          
#>                  95% CI : (0.8167, 0.8439)
#>     No Information Rate : 0.7759          
#>     P-Value [Acc > NIR] : 7.273e-14       
#>                                           
#>                   Kappa : 0.5023          
#>  Mcnemar's Test P-Value : 0.08357         
#>                                           
#>             Sensitivity : 0.8994          
#>             Specificity : 0.5923          
#>          Pos Pred Value : 0.8842          
#>          Neg Pred Value : 0.6297          
#>              Prevalence : 0.7759          
#>          Detection Rate : 0.6979          
#>    Detection Prevalence : 0.7893          
#>       Balanced Accuracy : 0.7459          
#>                                           
#>        'Positive' Class : no              
#> 
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics[width=1.1\linewidth]{main_files/figure-latex/plot_rf_ROC-1} 

}

\caption[Random Forest Model AUC and ROC Curve]{Random Forest Model AUC and ROC Curve}\label{fig:plot_rf_ROC}
\end{figure}
\end{Schunk}

\hypertarget{logistic-regression-model}{%
\subsection{Logistic Regression Model}\label{logistic-regression-model}}

Logistic regression is an efficient, interpretable and reasonably
accurate method, which fits quickly with minimal tuning. Logistic
regression prediction accuracy will benefit if the data is close to
Gaussian distribution. Thus we apply addition transformation to the
training data set. We will also be employing 5-fold cross validation
resampling procedure to improve the model. In addition to the above we
are going to convert **Location* categorical value to numeric data type.
We could have used dummy encoding but having 49 locations this approach
does not seem beneficial.

\begin{Schunk}
\begin{Sinput}
confusionMatrix(data = pred.logRegModel.raw, testDataCopy$RainTomorrow)
\end{Sinput}
\begin{Soutput}
#> Confusion Matrix and Statistics
#> 
#>           Reference
#> Prediction    0    1
#>          0 1859  167
#>          1  468  505
#>                                           
#>                Accuracy : 0.7883          
#>                  95% CI : (0.7732, 0.8028)
#>     No Information Rate : 0.7759          
#>     P-Value [Acc > NIR] : 0.05426         
#>                                           
#>                   Kappa : 0.4748          
#>  Mcnemar's Test P-Value : < 2e-16         
#>                                           
#>             Sensitivity : 0.7989          
#>             Specificity : 0.7515          
#>          Pos Pred Value : 0.9176          
#>          Neg Pred Value : 0.5190          
#>              Prevalence : 0.7759          
#>          Detection Rate : 0.6199          
#>    Detection Prevalence : 0.6756          
#>       Balanced Accuracy : 0.7752          
#>                                           
#>        'Positive' Class : 0               
#> 
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics[width=1.1\linewidth]{main_files/figure-latex/plot_logReg_ROC-1} 

}

\caption[Logistic Regression Model AUC and ROC Curve]{Logistic Regression Model AUC and ROC Curve}\label{fig:plot_logReg_ROC}
\end{figure}
\end{Schunk}

Confusion matrix and Figure \ref{fig:plot_logReg_ROC} demonstrate the
logistic model performance on the balanced data set. Using the
proportion of positive data points that are correctly considered as
positive (true positives) and the proportion of negative data points
that are mistakenly considered as positive (false negative), we
generated a graphic that shows the trade off between the rate at which
the model correctly predicts the rain tomorrow with the rate of
incorrectly predicting the rain. The value around 0.80 indicates that
the model does a good job in discriminating between the two categories.

\hypertarget{model-comparison}{%
\subsection{Model Comparison}\label{model-comparison}}

Not it is time to compare the models side by side and pick a winner.

\begin{Schunk}
\begin{figure}[H]

{\centering \includegraphics[width=1.1\linewidth]{main_files/figure-latex/plot_model_comp-1} 

}

\caption[Model Accuracy Comparison]{Model Accuracy Comparison}\label{fig:plot_model_comp}
\end{figure}
\begin{Soutput}
#>               model       auc
#> 1       logRegModel 0.7751854
#> 2    classTreeModel 0.7555773
#> 3   naiveBayesModel 0.7537090
#> 4 randomForestModel 0.7445784
\end{Soutput}
\end{Schunk}

Let's review model metrics.

\hypertarget{auc---roc-perfomance}{%
\subsubsection{AUC - ROC perfomance}\label{auc---roc-perfomance}}

AUC stands for Area under the ROC Curve and ROC for Receiver operating
characteristic curve. This is one of the most important KPIs of the
classification algorithms. This two metrics measure how well the models
distinguishing between the classes. separates positive and negative
outcome. The higher AUC the better model predicts positive and negative
outcome.

Figures \ref{fig:plot_classTree_ROC}, \ref{fig:plot_nb_ROC},
\ref{fig:plot_rf_ROC}, \ref{fig:plot_logReg_ROC} and accompanying data
show that on the test data set the Random Forest model has the higher
overall accuracy (83\%) but performs poorly predicting rainy days
(59\%), thus balanced accuracy is lower (about 75\%).

Naive Bayes and Decision Tree are less accurate models in comparison
with the Random Forest one but way more balanced demonstrating
consistent power to predict rainy and sunny days with equal accuracy.
They have balanced accuracy of 74\% and 79\% respectively.

Logistic regression model scores the best having the highest AUC and all
other metrics around 80\%.

\hypertarget{model-interpretibility}{%
\subsubsection{Model interpretibility}\label{model-interpretibility}}

Logistic Regression, Decision Tree and Naive Bayes are all highly
interpreatable models. It is easy to explain to the business what impact
each input parameter has. The decision tree could be even visualized (
if it is not too large).

Random Forest on the other hand is a black-box model, complex algorithm
which is difficult to explain in simple terms.

\hypertarget{data-preparation-1}{%
\subsubsection{Data Preparation}\label{data-preparation-1}}

Decision Tree, Random Forest and Naive Bayes can deal with missing data,
outliers, numeric and alphanumeric values. In simple terms they are not
very demanding for data quality. It would be interesting to see how they
perform on the original data set without data cleaning. but this is
subject of another research.

Logistic regression does require conversion of alphanumeric values to
numeric, struggles dealing with the outliers and performs best when
dealing with the data that have normal distribution.

\hypertarget{verdict-to-be-finalized}{%
\subsubsection{Verdict (To be
finalized)}\label{verdict-to-be-finalized}}

Despite sensitivity to data quality Logistic regression outperforms
other models in all other categories.

\hypertarget{model-deployment}{%
\section{Model Deployment}\label{model-deployment}}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\hypertarget{bibliography}{%
\section{Bibliography}\label{bibliography}}

\newpage

\hypertarget{note-from-the-authors}{%
\section{Note from the Authors}\label{note-from-the-authors}}

This file was generated using
\href{https://github.com/rstudio/rticles}{\emph{The R Journal} style
article template}, additional information on how to prepare articles for
submission is here -
\href{https://journal.r-project.org/share/author-guide.pdf}{Instructions
for Authors}. The article itself is an executable R Markdown file that
could be
\href{https://github.com/ivbsoftware/big-data-final-2/blob/master/docs/R_Journal/big-data-final-2/}{downloaded
from Github} with all the necessary artifacts.


\address{%
Sumaira Afzal\\
York University School of Continuing Studies\\
\\
}


\address{%
Viraja Ketkar\\
York University School of Continuing Studies\\
\\
}


\address{%
Murlidhar Loka\\
York University School of Continuing Studies\\
\\
}


\address{%
Vadim Spirkov\\
York University School of Continuing Studies\\
\\
}


